import json
from pathlib import Path
from typing import Callable, Dict, List

from langchain_community.document_loaders import (
    DirectoryLoader,
    PyPDFLoader,
    TextLoader,
    UnstructuredHTMLLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredPowerPointLoader,
    WebBaseLoader,
)
from langchain_core.documents import Document


# =====================================================
# Paths
# =====================================================
RAW_DIR = Path(
    "E:/OneDrive/Documents/GitHub/sourceGroundedAnswer/data/raw/"
)

PROCESSED_DIR = Path(
    "E:/OneDrive/Documents/GitHub/sourceGroundedAnswer/data/processed/"
)
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

OUTPUT_FILE = PROCESSED_DIR / "all_documents.json"


# =====================================================
# Base pipeline stage (Stage 1: load + normalize)
# =====================================================
class MetadataLoaderStage1:
    def load(self) -> List[Document]:
        docs = self._load()

        # normalize metadata
        for d in docs:
            source = d.metadata.get("source", "")
            p = Path(source)

            d.metadata["file_name"] = p.name
            d.metadata["format"] = p.suffix.lstrip(".").lower()
            d.metadata.setdefault("source_type", "file")

        return docs

    def _load(self) -> List[Document]:
        raise NotImplementedError

    def save_all(self, docs: List[Document]) -> None:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            json.dump(
                [
                    {
                        "page_content": d.page_content,
                        "metadata": d.metadata,
                    }
                    for d in docs
                ],
                f,
                indent=2,
                ensure_ascii=False,
            )

    def load_all(self) -> List[Document]:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        return [
            Document(
                page_content=item["page_content"],
                metadata=item["metadata"],
            )
            for item in data
        ]

    def preview(self, docs: List[Document], chars: int = 200) -> None:
        print(f"\nTotal documents: {len(docs)}\n")
        for i, d in enumerate(docs):
            text = (d.page_content or "")[:chars].replace("\n", " ")
            print(f"[{i}] {text}...")
            print(f"    metadata: {d.metadata}\n")


# =====================================================
# Loader factories (FILES + URL at same abstraction level)
# =====================================================
def make_directory_loader(
    glob_pattern: str,
    loader_cls,
) -> Callable[[], List[Document]]:
    def load():
        return DirectoryLoader(
            path=str(RAW_DIR),
            glob=glob_pattern,
            loader_cls=loader_cls,
        ).load()

    return load


def make_url_loader(urls: List[str]) -> Callable[[], List[Document]]:
    def load():
        return WebBaseLoader(urls).load()

    return load


# =====================================================
# Source registry (single source of truth)
# =====================================================
SOURCE_LOADERS: Dict[str, Callable[[], List[Document]]] = {
    "pdf": make_directory_loader("**/*.pdf", PyPDFLoader),
    "pptx": make_directory_loader("**/*.pptx", UnstructuredPowerPointLoader),
    "txt": make_directory_loader("**/*.txt", TextLoader),
    "html": make_directory_loader("**/*.html", UnstructuredHTMLLoader),
    "docx": make_directory_loader("**/*.docx", UnstructuredWordDocumentLoader),
    "url": make_url_loader(
        [
            "https://www.sciencenews.org/",
            "https://www.sciencedaily.com/",
        ]
    ),
}


# =====================================================
# Unified loader (no special cases)
# =====================================================
class AllSourceLoader(MetadataLoaderStage1):
    def __init__(self, sources: List[str] | None = None):
        # None = load all registered sources
        self.sources = sources or SOURCE_LOADERS.keys()

    def _load(self) -> List[Document]:
        docs: List[Document] = []

        for source_name in self.sources:
            loader_fn = SOURCE_LOADERS[source_name]
            docs.extend(loader_fn())

        return docs


# =====================================================
# Entry point
# =====================================================
if __name__ == "__main__":
    # Examples:
    # loader = AllSourceLoader()                 # everything
    # loader = AllSourceLoader(["pdf"])          # only PDFs
    # loader = AllSourceLoader(["pdf", "url"])   # PDFs + URLs

    loader = AllSourceLoader()

    docs = loader.load()
    loader.preview(docs)
    loader.save_all(docs)

    print(f"\nSaved {len(docs)} documents to:")
    print(OUTPUT_FILE)
